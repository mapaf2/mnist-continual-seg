{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3ywe2AJJYe60"},"source":["# Abstract\n","This tutorial aims to be a simple starter kit for people wanting to experiment continual semantic segmentation. To simplify experiments and developments of methods, we propose Continual-Extended-Mnist, a new toy dataset for continual segmentation that reflects the same scenarios explored in recent papers. We explain two of the important challenges, namely catastrophic forgetting coupled with background shift, and show how a naïve baseline systematically fails even on our black-and-white low-resolution images. Then, we introduce a few tricks popularized by the papers “Knowledge Distillation for Incremental Learning in Semantic Segmentation” and “Modeling the Background for Incremental Learning in Semantic Segmentation” (MiB)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mjW95QruSZYr"},"source":["# Introduction\n","If we look at papers on continual learning for classification, most of them experiment on simple datasets such as MNIST, CIFAR10 or Mini-Imagenet. These are small-scale datasets of tiny images which are used to experiment on a relatively simple task: classify object-centered images. On the other hand, the datasets employed to explore the more complex task of continual semantic segmentation are generally Pascal-VOC, ADE20K or COCO: ~10K to 20K high-resolution images with many objects in each image, which requires several hours of training on GPUs, if not days.\n","\n","While toy datasets are sometimes blamed to be unrealistic as they are much more simple than real-life scenarios, they serve a non negligeable purpose. **They accelerate the research cycle and development of models** as they remove unnecessary complicated variables while keeping most of the essentials. It allows to do many runs of experiments with different models in a fraction of the time to see if it addresses the problems we aim to solve, i.e. in our case catastrophic forgetting especially. If a technique cannot reduce catastrophic forgetting on 60x60 black-and-white images, obviously there is no reason to think that it will solve it on real images of complex scenes.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Smmx3YHSp4AS"},"source":["# Continual Extended Mnist"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"r2mxHVLMqDLR"},"source":["We owe the generation of the raw data to this [repository](https://github.com/LukeTonin/simple-deep-learning) made by Luke Tonin. He also made an excellent tutorial for [simple semantic segmentation ](https://awaywithideas.com/mnist-extended-a-dataset-for-semantic-segmentation-and-object-detection/) that was an inspiration for ours. We started from this dataset and adapted it to a class-incremental learning scenario, i.e. classes are seen sequentially in successive tasks. In the first task, the model has to learn to segment 0's and 1's, then in the next task it must learn 2's and 3's, then 4's and 5's, and so on. \n","\n","Let's first take a look at the dataset."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68455,"status":"ok","timestamp":1633619556433,"user":{"displayName":"Mathieu PAGÉ FORTIN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09654843651526877985"},"user_tz":240},"id":"MvdVW-qczRPx","outputId":"e5e5a57c-bf91-44c9-e9cb-fd3dfd03f433"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["#from google.colab import drive\n","#drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21373,"status":"ok","timestamp":1633619577803,"user":{"displayName":"Mathieu PAGÉ FORTIN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09654843651526877985"},"user_tz":240},"id":"RpMiEQp4YeLs","outputId":"37601d65-ae52-4efe-c11d-2b12b0c69aea"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-06-07 14:31:24.818040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"]}],"source":["import os\n","import sys\n","#os.chdir(\"/content/gdrive/MyDrive/Colab Notebooks/mnist_continual_seg\")\n","sys.path.append(\"/home/mathieu/Documents/Deep/mnist-continual-seg/simple-deep-learning\")\n","import matplotlib.pyplot as plt\n","%matplotlib inline  \n","from scenarios import ContinualMnist\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Initialize dataloader, optimizer and trainer\n","from models import simple_seg_model\n","_tasks = {0: [0,1], 1: [2,3], 2: [4,5], 3: [6,7], 4: [8,9]}\n","model = simple_seg_model(n_classes_per_task=[len(_tasks[0])+1])\n","model = model.cuda()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","from metrics import EvaluaterCallback\n","from trainer import Trainer\n","\n","# Set optimizer\n","optimizer = torch.optim.Adam(lr = 0.0005, params=model.parameters())\n","# Set scenario\n","continual_mnist = ContinualMnist(n_train=5000, n_test=2500, batch_size=72, tasks=_tasks)\n","# Set Evaluator\n","evaluater = EvaluaterCallback(model, [\"confusion_matrix\"], callback_frequency=\"epoch\", n_classes=11, save_matrices=True)\n","# Set trainer\n","trainer = Trainer(model,\n","                  n_classes=[3],\n","                  optim=optimizer,\n","                  callbacks=[evaluater])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["*******\n","Task #0\n","*******\n","Classes to learn:\n","-1 0 1\n","*******\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]/home/mathieu/.conda/envs/mmdet2.0/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","100%|██████████| 100/100 [01:51<00:00,  1.11s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Overall stats\n","|    |   Overall Acc |   Mean Acc |   Mean IoU |\n","|---:|--------------:|-----------:|-----------:|\n","|  0 |      0.992256 |   0.946277 |   0.893905 |\n","Class IoU\n","|    |       -1 |        0 |        1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   |\n","|---:|---------:|---------:|---------:|:----|:----|:----|:----|:----|:----|:----|:----|\n","|  0 | 0.991974 | 0.839183 | 0.850556 | X   | X   | X   | X   | X   | X   | X   | X   |\n","Class Acc\n","|    |       -1 |        0 |        1 | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   |\n","|---:|---------:|---------:|---------:|:----|:----|:----|:----|:----|:----|:----|:----|\n","|  0 | 0.995685 | 0.917666 | 0.925482 | X   | X   | X   | X   | X   | X   | X   | X   |\n","\n","####################################\n","Next Task\n","####################################\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["*******\n","Task #1\n","*******\n","Classes to learn:\n","-1 2 3\n","*******\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [01:49<00:00,  1.09s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Overall stats\n","|    |   Overall Acc |   Mean Acc |   Mean IoU |\n","|---:|--------------:|-----------:|-----------:|\n","|  0 |      0.963796 |   0.552332 |   0.516122 |\n","Class IoU\n","|    |       -1 |   0 |   1 |        2 |        3 | 4   | 5   | 6   | 7   | 8   | 9   |\n","|---:|---------:|----:|----:|---------:|---------:|:----|:----|:----|:----|:----|:----|\n","|  0 | 0.963059 |   0 |   0 | 0.790388 | 0.827166 | X   | X   | X   | X   | X   | X   |\n","Class Acc\n","|    |      -1 |   0 |   1 |        2 |        3 | 4   | 5   | 6   | 7   | 8   | 9   |\n","|---:|--------:|----:|----:|---------:|---------:|:----|:----|:----|:----|:----|:----|\n","|  0 | 0.99693 |   0 |   0 | 0.867415 | 0.897315 | X   | X   | X   | X   | X   | X   |\n","\n","####################################\n","Next Task\n","####################################\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["*******\n","Task #2\n","*******\n","Classes to learn:\n","-1 4 5\n","*******\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [01:55<00:00,  1.15s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Overall stats\n","|    |   Overall Acc |   Mean Acc |   Mean IoU |\n","|---:|--------------:|-----------:|-----------:|\n","|  0 |      0.940266 |   0.398478 |   0.370336 |\n","Class IoU\n","|    |       -1 |   0 |   1 |   2 |   3 |        4 |        5 | 6   | 7   | 8   | 9   |\n","|---:|---------:|----:|----:|----:|----:|---------:|---------:|:----|:----|:----|:----|\n","|  0 | 0.939326 |   0 |   0 |   0 |   0 | 0.835262 | 0.817766 | X   | X   | X   | X   |\n","Class Acc\n","|    |       -1 |   0 |   1 |   2 |   3 |        4 |       5 | 6   | 7   | 8   | 9   |\n","|---:|---------:|----:|----:|----:|----:|---------:|--------:|:----|:----|:----|:----|\n","|  0 | 0.998161 |   0 |   0 |   0 |   0 | 0.894935 | 0.89625 | X   | X   | X   | X   |\n","\n","####################################\n","Next Task\n","####################################\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["*******\n","Task #3\n","*******\n","Classes to learn:\n","-1 6 7\n","*******\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [02:02<00:00,  1.23s/it]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Overall stats\n","|    |   Overall Acc |   Mean Acc |   Mean IoU |\n","|---:|--------------:|-----------:|-----------:|\n","|  0 |      0.921012 |   0.314014 |   0.287986 |\n","Class IoU\n","|    |       -1 |   0 |   1 |   2 |   3 |   4 |   5 |       6 |        7 | 8   | 9   |\n","|---:|---------:|----:|----:|----:|----:|----:|----:|--------:|---------:|:----|:----|\n","|  0 | 0.920241 |   0 |   0 |   0 |   0 |   0 |   0 | 0.85159 | 0.820039 | X   | X   |\n","Class Acc\n","|    |       -1 |   0 |   1 |   2 |   3 |   4 |   5 |        6 |       7 | 8   | 9   |\n","|---:|---------:|----:|----:|----:|----:|----:|----:|---------:|--------:|:----|:----|\n","|  0 | 0.998729 |   0 |   0 |   0 |   0 |   0 |   0 | 0.914137 | 0.91326 | X   | X   |\n","\n","####################################\n","Next Task\n","####################################\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["*******\n","Task #4\n","*******\n","Classes to learn:\n","-1 8 9\n","*******\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [01:59<00:00,  1.20s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","Overall stats\n","|    |   Overall Acc |   Mean Acc |   Mean IoU |\n","|---:|--------------:|-----------:|-----------:|\n","|  0 |      0.898288 |   0.258688 |   0.230222 |\n","Class IoU\n","|    |      -1 |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |        8 |        9 |\n","|---:|--------:|----:|----:|----:|----:|----:|----:|----:|----:|---------:|---------:|\n","|  0 | 0.89723 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 | 0.822529 | 0.812685 |\n","Class Acc\n","|    |       -1 |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |       8 |        9 |\n","|---:|---------:|----:|----:|----:|----:|----:|----:|----:|----:|--------:|---------:|\n","|  0 | 0.997878 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 | 0.94066 | 0.907034 |\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from utils import meta_train\n","meta_train(n_tasks = len(_tasks),\n","           epochs = 100,\n","           scenario = continual_mnist, \n","           trainer=trainer,\n","           evaluater=evaluater,\n","           memory=None,\n","           animation_path='naive')"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["evaluater.create_animation('experiments/2-2/1000/72/100/fine-tuning/ft', sample_freq=4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN3Zt4rLkQv16UA3//5asCq","collapsed_sections":[],"name":"Tutorial_continual_segmentation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
